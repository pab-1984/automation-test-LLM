// runners/adapters/ollama.adapter.js
// Adapter para conectar con Ollama (LLM local)

const fs = require('fs');

class OllamaAdapter {
  constructor(config) {
    this.config = config;
    this.baseUrl = config.baseUrl || 'http://localhost:11434';
    this.model = config.model || 'llama3.2:3b';
    this.temperature = config.temperature || 0.1;
    this.maxTokens = config.maxTokens || 2000;
  }

  async initialize() {
    console.log(`üîå Conectando con Ollama en ${this.baseUrl}...`);
    
    // Verificar que Ollama est√© disponible
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`);
      
      if (!response.ok) {
        throw new Error(`Ollama respondi√≥ con status ${response.status}`);
      }
      
      const data = await response.json();
      
      // Verificar que el modelo existe
      const modelExists = data.models && data.models.some(m => m.name === this.model);
      
      if (!modelExists) {
        console.log(`\n‚ö†Ô∏è  Modelo ${this.model} no encontrado.`);
        console.log('Modelos disponibles:');
        if (data.models && data.models.length > 0) {
          data.models.forEach(m => console.log(`   - ${m.name}`));
        } else {
          console.log('   (ninguno)');
        }
        console.log(`\nPara instalar el modelo, ejecuta:`);
        console.log(`   ollama pull ${this.model}\n`);
        throw new Error(`Modelo ${this.model} no est√° instalado`);
      }
      
      console.log(`‚úÖ Conectado a Ollama`);
      console.log(`   Modelo: ${this.model}`);
      console.log(`   Temperatura: ${this.temperature}`);
      
    } catch (error) {
      if (error.message.includes('fetch failed') || error.code === 'ECONNREFUSED') {
        throw new Error(
          '‚ùå No se pudo conectar a Ollama.\n' +
          '   Aseg√∫rate de que Ollama est√© corriendo:\n' +
          '   1. Abre otra terminal\n' +
          '   2. Ejecuta: ollama serve\n' +
          '   3. Vuelve a intentar'
        );
      }
      throw error;
    }
  }

  async processStep(prompt, context) {
    try {
      // Construir el prompt final
      const fullPrompt = this.buildPrompt(prompt, context);
      
      console.log(`   ü§ñ Consultando a Ollama...`);
      
      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: this.model,
          prompt: fullPrompt,
          stream: false,
          options: {
            temperature: this.temperature,
            num_predict: this.maxTokens,
            top_p: 0.9,
            top_k: 40
          }
        })
      });

      if (!response.ok) {
        throw new Error(`Ollama error: ${response.status} ${response.statusText}`);
      }

      const data = await response.json();
      
      if (!data.response) {
        throw new Error('Ollama no retorn√≥ respuesta');
      }

      console.log(`   üí≠ Respuesta recibida (${data.response.length} caracteres)`);
      
      // Intentar parsear JSON de la respuesta
      const parsed = this.parseResponse(data.response);
      
      if (parsed) {
        console.log(`   üéØ Acci√≥n interpretada: ${parsed.action}`);
        return parsed;
      }

      // Si no hay JSON v√°lido, usar fallback
      console.log('   ‚ö†Ô∏è  Respuesta no JSON, usando fallback');
      return this.fallbackResponse(context.step);
      
    } catch (error) {
      console.error(`   ‚ùå Error en Ollama: ${error.message}`);
      
      // En caso de error, ejecutar directo sin IA
      return this.fallbackResponse(context.step);
    }
  }

  buildPrompt(basePrompt, context) {
    // Usar prompt simplificado si existe, sino el original
    let simplePrompt = basePrompt;
    try {
      if (fs.existsSync('./prompts/system-simple.md')) {
        simplePrompt = fs.readFileSync('./prompts/system-simple.md', 'utf8');
      }
    } catch (e) {
      // Usar el prompt original si hay error
      console.log('   ‚ö†Ô∏è  No se pudo cargar prompt simplificado, usando original');
    }
    
    return `${simplePrompt}

## Tarea actual:
Acci√≥n solicitada: ${context.step.action}
Descripci√≥n: ${context.step.description || 'Sin descripci√≥n'}
Par√°metros completos: ${JSON.stringify(context.step, null, 2)}

## Contexto:
URL actual: ${context.currentUrl}
Base URL: ${context.baseUrl}

Responde SOLO con JSON v√°lido usando el formato exacto mostrado arriba:`;
  }

  parseResponse(responseText) {
    try {
      // Intentar parsear como JSON directo
      const parsed = JSON.parse(responseText);
      if (parsed.action && parsed.params) {
        return parsed;
      }
    } catch (e) {
      // No es JSON directo, intentar extraer
    }

    // Intentar extraer JSON del texto
    const jsonMatch = responseText.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      try {
        const parsed = JSON.parse(jsonMatch[0]);
        if (parsed.action && parsed.action !== 'navigate') { // Evitar navegaciones incorrectas
          return parsed;
        }
      } catch (e) {
        // Fall√≥ el parsing
      }
    }

    // Intentar extraer de bloques de c√≥digo
    const codeBlockMatch = responseText.match(/```(?:json)?\s*(\{[\s\S]*?\})\s*```/);
    if (codeBlockMatch) {
      try {
        const parsed = JSON.parse(codeBlockMatch[1]);
        if (parsed.action && parsed.action !== 'navigate') { // Evitar navegaciones incorrectas
          return parsed;
        }
      } catch (e) {
        // Fall√≥ el parsing
      }
    }

    return null;
  }

  fallbackResponse(step) {
    // Respuesta de emergencia cuando el LLM no responde correctamente
    console.log(`   üîÑ Ejecutando acci√≥n directa del YAML`);
    
    // Crear una copia limpia de los par√°metros
    const params = { ...step };
    delete params.action;
    delete params.description;
    delete params.mode;
    
    return {
      action: step.action,
      params: params,
      reasoning: 'Fallback - ejecutando acci√≥n directa sin interpretaci√≥n del LLM'
    };
  }

  async cleanup() {
    // Ollama no necesita limpieza espec√≠fica
    console.log('‚úì Ollama adapter limpiado');
  }
}

module.exports = OllamaAdapter;
